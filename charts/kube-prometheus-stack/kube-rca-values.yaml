alertmanager:
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-type: "basic"
      nginx.ingress.kubernetes.io/auth-secret: "basic-auth"
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
      external-dns.alpha.kubernetes.io/target: 121.130.214.237
    hosts:
      - alertmanager.kkamji.net
    paths:
      - /
    tls:
      - secretName: alertmanager-tls
        hosts:
          - alertmanager.kkamji.net
  config:
    global:
      resolve_timeout: 5m
    route:
      receiver: 'kube-rca-backend'
      group_by: ['alertname', 'namespace', 'severity']
      group_wait: 10s
      group_interval: 1m
      repeat_interval: 4h
      routes:
        # Watchdog 예외처리
        - receiver: 'null'
          matchers:
            - alertname = "Watchdog"
        # Cilium 사용, kube-proxy 예외처리
        - receiver: 'null'
          matchers:
            - alertname = "KubeProxyDown"
        # API server 버전 낮음, 임시 예외처리
        - receiver: 'null'
          matchers:
            - alertname = "KubeVersionMismatch"
        - receiver: 'kube-rca-backend'
          matchers:
            - alertname = "KubePodContainerStatusKilled"
        - receiver: 'kube-rca-backend'
          matchers:
            - alertname = "KubePodImagePullError"
    receivers:
      - name: 'null'
      - name: 'kube-rca-backend'
        webhook_configs:
          # 추후 svc, ns 이름 확정 후 수정 필요
          - url: 'http://kube-rca-backend.kube-rca.svc:8080/webhook/alertmanager'
            send_resolved: true

grafana:
  admin:
    existingSecret: grafana-auth
    userKey: admin-user
    passwordKey: admin-password
  defaultDashboardsTimezone: browser # TimeZone 설정 (default: UTC)
  ingress:
    ingressClassName: nginx
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      external-dns.alpha.kubernetes.io/target: 121.130.214.237
    hosts:
    - grafana.kkamji.net
    path: /
    pathType: Prefix
    tls:
    - secretName: grafana-tls
      hosts:
      - grafana.kkamji.net
  additionalDataSources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki-gateway:80
    jsonData:
      timeout: 60
      maxLines: 1000
      httpHeaderName1: "X-Scope-OrgID"
    secureJsonData:
      httpHeaderValue1: "kube-rca"
  sidecar:
    dashboards:
      folderAnnotation: "grafana_folder"
      defaultFolderName: "Default Dashboard"

prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
    # Hubble & Cilium Metric 수집을 위한 설정
    - job_name: 'kubernetes-pods' ## Cilium
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape ]
        action: keep
        regex: true
      - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: ${1}:${2}
        target_label: __address__

    - job_name: 'kubernetes-endpoints' ## Hubble
      scrape_interval: 30s
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape ]
        action: keep
        regex: true
      - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
        action: replace
        target_label: __address__
        regex: (.+)(?::\d+);(\d+)
        replacement: $1:$2

    # Istio Envoy sidecar metrics
    - job_name: 'envoy-stats'
      metrics_path: /stats/prometheus
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_container_port_name]
        action: keep
        regex: '.*-envoy-prom'
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod

    # Istiod control plane metrics
    - job_name: 'istiod'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - istio-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: istiod;http-monitoring
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod

    retention: 14d
  ingress:
    ingressClassName: nginx
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-type: "basic" # 인증 타입
      nginx.ingress.kubernetes.io/auth-secret: "basic-auth" # 인증에 사용할 secret
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required" # 인증 프롬프트에 보내는 메시지
      external-dns.alpha.kubernetes.io/target: 121.130.214.237
    hosts:
    - prometheus.kkamji.net
    paths:
    - /
    tls:
    - secretName: prometheus-tls
      hosts:
      - prometheus.kkamji.net

defaultRules:
  disabled:
    KubePodCrashLooping: true

additionalPrometheusRulesMap:
  kube-rca-crashlooping:
    groups:
    - name: kube-rca-crashlooping
      rules:
      - alert: KubePodCrashLooping
        expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: Pod is crash looping.
          description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in CrashLoopBackOff.'
  kube-rca-container-status-killed:
    groups:
    - name: kube-rca-container-status-killed
      rules:
      - alert: KubePodContainerStatusKilled
        expr: |
          (
            increase(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[5m]) > 0
            and on (namespace, pod, container)
            kube_pod_container_status_last_terminated_exitcode{job="kube-state-metrics"} == 137
          )
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Pod container killed (SIGKILL)
          description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) was killed with SIGKILL (exit code 137). Possible causes: OOMKilled, liveness probe failure, resource limits, or forced termination.'

  kube-rca-image-pull-error:
    groups:
    - name: kube-rca-image-pull-error
      rules:
      - alert: KubePodImagePullError
        expr: |
          max_over_time(kube_pod_container_status_waiting_reason{reason=~"ErrImagePull|ImagePullBackOff", job="kube-state-metrics"}[5m]) >= 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: Pod container ImagePullError
          description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) failed to pull image.'
